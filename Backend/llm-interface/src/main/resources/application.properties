quarkus.langchain4j.log-requests=true
quarkus.langchain4j.log-responses=true

#quarkus.log.level=DEBUG
quarkus.log.console.enable=true

#dream model to be used
#quarkus.langchain4j.huggingface.chat-model.inference-endpoint-url=https://api-inference.huggingface.co/models/google/gemma-3-27b-it
quarkus.langchain4j.huggingface.chat-model.wait-for-model=true
quarkus.langchain4j.huggingface.chat-model.enabled=true
quarkus.langchain4j.huggingface.chat-model.return-full-text=false
quarkus.langchain4j.huggingface.timeout=3m


#quarkus.langchain4j.huggingface.chat-model.inference-endpoint-url=https://api-inference.huggingface.co/models/ZySec-AI/gemma-3-27b-tools
#quarkus.langchain4j.huggingface.chat-model.inference-endpoint-url=https://api-inference.huggingface.co/models/ibm-granite/granite-3.2-2b-instruct
#quarkus.langchain4j.huggingface.chat-model.inference-endpoint-url=https://api-inference.huggingface.co/models/PartAI/Dorna-Llama3-8B-Instruct
#quarkus.langchain4j.huggingface.chat-model.inference-endpoint-url=https://api-inference.huggingface.co/models/meta-llama/Llama-3.1-8B-Instruct

### Chat model configurationsmeta-llama/Llama-3.2-3B-Instruct
# Activate or not the Mistral AI chat model
#quarkus.langchain4j.huggingface.chat-model.enabled=true             
# Chat model name used
#quarkus.langchain4j.mistralai.chat-model.model-name=Mistral-7B-Instruct-v0.2
# Number of tokens to use
#quarkus.langchain4j.huggingface.chat-model.max-new-tokens=250


#quarkus.langchain4j.infinispan.dimension=384
quarkus.langchain4j.pgvector.dimension=384
%dev.quarkus.langchain4j.pgvector.register-vector-pg-extension=false
quarkus.langchain4j.pgvector.register-vector-pg-extension=false
#quarkus.langchain4j.huggingface.embedding-model.inference-endpoint-url=https://api-inference.huggingface.co/models/jinaai/jina-embeddings-v3

#%prod.quarkus.infinispan-client.uri=hotrod://admin:admin@localhost:11222
#%prod.quarkus.infinispan-client.client-intelligence=BASIC 

quarkus.langchain4j.embedding-model.provider=dev.langchain4j.model.embedding.onnx.bgesmallenq.AllMiniLmL6V2QuantizedEmbeddingModel

rag.max-results=2
#quarkus.langchain4j.easy-rag.path=src/main/resources/rag

#quarkus.infinispan-client.devservices.enabled=true
#quarkus.langchain4j.chat-model.provider=openai

#-------------------     PgVector Store Custom Properties     ---------------------------#
#pgvector.database.host=localhost
pgvector.database.user=admin
pgvector.database.password=admin
#pgvector.database.table=embeddings


quarkus.langchain4j."llama".chat-model.provider=ollama
quarkus.langchain4j."meta-llama".chat-model.provider=huggingface
quarkus.langchain4j."oai".chat-model.provider=openai


use.openai=${USE_OPENAI:false}
quarkus.langchain4j.openai."oai".api-key=${OPENAI_API_CHATBOT_KEY:yourkey}
quarkus.langchain4j.openai."oai".base-url=${OPENAI_API_CHATBOT_BASE_URL:https://api.openai.com/v1}
quarkus.langchain4j.openai."oai".chat-model.model-name=${OPENAI_API_CHATBOT_MODEL_NAME:gpt-4o-mini}

#quarkus.langchain4j.ollama.llama.base-url=${langchain4j-ollama-dev-service.ollama.endpoint}
quarkus.langchain4j.ollama."llama".chat-model.model-id=${QUARKUS_LANGCHAIN4J_OLLAMA__LLAMA__CHAT_MODEL_MODEL_ID:qwen3:14b}
quarkus.langchain4j.ollama.devservices.enabled=false
quarkus.langchain4j.ollama."llama".log-requests=true
quarkus.langchain4j.ollama."llama".log-responses=true
quarkus.langchain4j.ollama."llama".timeout=2m
quarkus.langchain4j.ollama."llama".chat-model.temperature=0

quarkus.langchain4j.huggingface."meta-llama".api-key=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


#---------------------    RabbitMQ Client ---------------------------------#
#quarkus.rabbitmqclient.virtual-host=/
#quarkus.rabbitmqclient.username=user
#quarkus.rabbitmqclient.password=password
#quarkus.rabbitmqclient.hostname=localhost
#quarkus.rabbitmqclient.port=5672


quarkus.http.port=8082

quarkus.http.cors=true
quarkus.http.cors.origins=*
quarkus.http.cors.methods=GET, POST, PUT, DELETE, OPTIONS, PATCH
quarkus.http.cors.headers=accept, authorization, content-type, x-requested-with